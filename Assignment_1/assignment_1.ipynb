{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Responsibility Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a data science related class in Python that violates the Single Responsibility Principle (SRP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_data(self):\n",
    "        return pd.read_csv(self.file_path)\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        # Perform data preprocessing tasks (e.g., handle missing values, encode categorical variables)\n",
    "        return data.dropna().astype(float)\n",
    "\n",
    "    def split_data(self, data):\n",
    "        X = data.drop('target', axis=1)\n",
    "        y = data['target']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "data_loader = DataLoader('..\\\\data\\\\sdg_electricity_data.csv')\n",
    "data = data_loader.load_data()\n",
    "# data = data_loader.preprocess_data(data)\n",
    "# X_train, X_test, y_train, y_test = data_loader.split_data(data)\n",
    "# model = data_loader.train_model(X_train, y_train)\n",
    "# mse = data_loader.evaluate_model(model, X_test, y_test)\n",
    "# print(f'Mean Squared Error: {mse:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the DataLoader class has multiple responsibilities:\n",
    "1. Loading data from a file\n",
    "2. Preprocessing data\n",
    "3. Splitting data into training and testing sets\n",
    "4. Training a machine learning model\n",
    "5. Evaluating the performance of the model\n",
    "\n",
    "This violates the Single Responsibility Principle, which states that a class should have only one reason to change. In this case, the `DataLoader` class has multiple reasons to change, making it difficult to maintain and extend.\n",
    "\n",
    "To fix this violation, we can break down the DataLoader class into separate classes, each with a single responsibility:\n",
    "- DataLoader: responsible for loading data from a file\n",
    "- DataPreprocessor: responsible for preprocessing data\n",
    "- DataSplitter: responsible for splitting data into training and testing sets\n",
    "- ModelTrainer: responsible for training a machine learning model\n",
    "- ModelEvaluator: responsible for evaluating the performance of the model\n",
    "\n",
    "By separating these responsibilities into individual classes, we can make the code more modular, maintainable, and scalable.\n",
    "\n",
    "Each class has a single responsibility, making the code more modular, maintainable, and scalable. Fix the SRP problem by breaking down the DataLoader class into separate classes, each with a single responsibility by completing the 4 TODOs below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class DataLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "def load_data(self):\n",
    "        return pd.read_csv(self.file_path)\n",
    "\n",
    "# data_preprocessor.py\n",
    "class DataPreprocessor:\n",
    "    def preprocess_data(self, data):\n",
    "        # TODO 1: Perform data preprocessing tasks (e.g., handle missing values, encode categorical variables)\n",
    "        pass\n",
    "\n",
    "# data_splitter.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "class DataSplitter:\n",
    "    def split_data(self, data):\n",
    "        # TODO 2: Split the data into training and testing sets\n",
    "        pass\n",
    "\n",
    "# model_trainer.py\n",
    "from sklearn.linear_model import LinearRegression\n",
    "class ModelTrainer:\n",
    "    def train_model(self, X_train, y_train):\n",
    "        # TODO 3: Train a machine learning model\n",
    "        pass\n",
    "\n",
    "# model_evaluator.py\n",
    "from sklearn.metrics import mean_squared_error\n",
    "class ModelEvaluatorClass:\n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        # TODO 4: Evaluate the model\n",
    "        pass\n",
    "\n",
    "# main.py\n",
    "data_loader = DataLoader('..\\\\data\\\\sdg_electricity_data.csv')\n",
    "data = data_loader.load_data()\n",
    "data_preprocessor = DataPreprocessor()\n",
    "data = data_preprocessor.preprocess_data(data)\n",
    "data_splitter = DataSplitter()\n",
    "X_train, X_test, y_train, y_test = data_splitter.split_data(data)\n",
    "model_trainer = ModelTrainer()\n",
    "model = model_trainer.train_model(X_train, y_train)\n",
    "model_evaluator = ModelEvaluatorClass()\n",
    "mse = model_evaluator.evaluate_model(model, X_test, y_test)\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-Closed Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a data science related class in Python that violates the Open-Closed Principle (OCP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "def analyze_data(self):\n",
    "        if self.data.shape[0] < 100:\n",
    "            return self._analyze_small_data()\n",
    "        elif self.data.shape[0] < 1000:\n",
    "            return self._analyze_medium_data()\n",
    "        else:\n",
    "            return self._analyze_large_data()\n",
    "\n",
    "def _analyze_small_data(self):\n",
    "        # Analyze small data using a simple method\n",
    "        return \"Small data analysis result\"\n",
    "\n",
    "def _analyze_medium_data(self):\n",
    "        # Analyze medium data using a moderately complex method\n",
    "        return \"Medium data analysis result\"\n",
    "\n",
    "def _analyze_large_data(self):\n",
    "        # Analyze large data using a complex method\n",
    "        return \"Large data analysis result\"\n",
    "\n",
    "data_analyzer = DataAnalyzer(some_data)\n",
    "result = data_analyzer.analyze_data()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the DataAnalyzer class has a single method analyze_data that analyzes the data based on its size. The method uses an if-elif-else statement to determine which analysis method to use, depending on the size of the data. \n",
    "\n",
    "This design violates the Open-Closed Principle because the DataAnalyzer class is not open for extension but closed for modification. If we want to add a new analysis method for a different data size, we would have to modify the existing analyze_data method, which breaks the OCP.\n",
    "\n",
    "To fix this violation, we can use polymorphism and inheritance to create a more flexible and extensible design. For example, we can create an abstract base class DataAnalyzer with an abstract method analyze_data, and then create concrete subclasses for each data size range that implement the analyze_data method. This way, we can add new analysis methods without modifying the existing code.\n",
    "\n",
    "You need to fix this violation by creating an abstract base class DataAnalyzer with an abstract method analyze_data, and then create concrete subclasses for each data size range that implement the analyze_data method. \n",
    "\n",
    "The create_data_analyzer function is used to create an instance of the appropriate subclass based on the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DataAnalyzer(ABC):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    @abstractmethod\n",
    "    def analyze_data(self):\n",
    "        pass\n",
    "\n",
    "# TODO: Create the three different classes for small, medium, and large data analysis\n",
    "\n",
    "def create_data_analyzer(data):\n",
    "    if data.shape[0] < 100:\n",
    "        return SmallDataAnalyzer(data)\n",
    "    elif data.shape[0] < 1000:\n",
    "        return MediumDataAnalyzer(data)\n",
    "    else:\n",
    "        return LargeDataAnalyzer(data)\n",
    "\n",
    "data_analyzer = create_data_analyzer(some_data)\n",
    "result = data_analyzer.analyze_data()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This design satisfies the Open-Closed Principle because we can add new analysis methods for different data size ranges without modifying the existing code. We can simply create a new subclass that implements the analyze_data method, and the create_data_analyzer function will take care of instantiating the correct subclass.\n",
    "This design is more flexible, extensible, and maintainable than the original code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liskov Substitution Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example of a data science related class in Python that violates the Liskov Substitution Principle (LSP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def load_data(self, file_path):\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "class CSVDataLoader(DataLoader):\n",
    "    def load_data(self, file_path):\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "class JSONDataLoader(DataLoader):\n",
    "    def load_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "data_loader = JSONDataLoader()\n",
    "data = data_loader.load_data('data.json')\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the JSONDataLoader class inherits from the DataLoader class, but it does not respect the contract defined by the DataLoader class. The load_data method in JSONDataLoader returns a pd.DataFrame object, whereas the load_data method in DataLoader returns a pd.DataFrame object or raises a ValueError if the file path is invalid.\n",
    "This violates the Liskov Substitution Principle because we cannot use a JSONDataLoader object in place of a DataLoader object without changing the behavior of the program. The JSONDataLoader class is not a true substitute for the DataLoader class.\n",
    "\n",
    "To fix this violation, we can modify the JSONDataLoader class to handle the file path in a way that is consistent with the contract defined by the DataLoader class. For example, we could modify the load_data method to raise a ValueError if the file path is invalid, or to return a default value if the file is empty.\n",
    "\n",
    "You need to fix this violation by modifying the JSONDataLoader class to handle the file path in a way that is consistent with the contract defined by the DataLoader class. Specifically, add a check to ensure that the file path ends with .json, and we raise a ValueError if it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def load_data(self, file_path):\n",
    "        if not file_path.endswith('.csv'):\n",
    "            raise ValueError(\"Invalid file path. Only CSV files are supported.\")\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "class CSVDataLoader(DataLoader):\n",
    "    def load_data(self, file_path):\n",
    "        # TODO 1: Implement loading data from a CSV file in a way that's consistent with the contract defined by the base class\n",
    "        pass\n",
    "\n",
    "class JSONDataLoader(DataLoader):\n",
    "    def load_data(self, file_path):\n",
    "        # TODO 2: Implement loading data from a JSON file in a way that's consistent with the contract defined by the base class\n",
    "        pass\n",
    "\n",
    "data_loader = JSONDataLoader()\n",
    "data = data_loader.load_data('data.json')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this modified code, we have modified the JSONDataLoader class to handle the file path in a way that is consistent with the contract defined by the DataLoader class. \n",
    "\n",
    "By modifying the JSONDataLoader class to handle the file path in a way that is consistent with the contract defined by the DataLoader class, we have fixed the Liskov Substitution Principle violation. We can now use the JSONDataLoader class in place of the DataLoader class without changing the behavior of the program.\n",
    "\n",
    "Note that we have also added a similar check to the DataLoader class to ensure that it only supports CSV files. This ensures that the DataLoader class and its subclasses have a consistent contract. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface Segregation Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a class that violates the Interface Segregation Principle (ISP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def train_linear_regression(self):\n",
    "        # implementation to train a linear regression model\n",
    "        pass\n",
    "\n",
    "    def train_logistic_regression(self):\n",
    "        # implementation to train a logistic regression model\n",
    "        pass\n",
    "\n",
    "    def train_decision_tree(self):\n",
    "        # implementation to train a decision tree model\n",
    "        pass\n",
    "\n",
    "    def train_random_forest(self):\n",
    "        # implementation to train a random forest model\n",
    "        pass\n",
    "\n",
    "    def train_neural_network(self):\n",
    "        # implementation to train a neural network model\n",
    "        pass\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        # implementation to evaluate the model\n",
    "        pass\n",
    "\n",
    "    def make_prediction(self):\n",
    "        # implementation to make a prediction using the model\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "model = Model(data)\n",
    "model.train_linear_regression()\n",
    "model.evaluate_model()\n",
    "model.make_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the Model class has a large interface that includes multiple methods for training different types of machine learning models (linear regression, logistic regression, decision tree, random forest, neural network), evaluating the model, and making predictions.\n",
    "\n",
    "The problem with this class is that it forces clients to depend on a large interface, even if they only need to use a subset of the methods. For example, a client that only needs to train a linear regression model and make predictions is forced to depend on the entire interface, including the methods for training other types of models and evaluating the model.\n",
    "\n",
    "This violates the Interface Segregation Principle, which states that \"clients should not be forced to depend on interfaces they don't use\". A better design would be to break down the interface into smaller, more focused interfaces that cater to specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By breaking down the interface into smaller, more focused interfaces, we can reduce the coupling between clients and the DataProcessor class, and make the system more modular and flexible. Fix the ISP violation by creating separate interfaces for LinearRegressionModel, LogisticRegressionModel and ModelEvaluator, based on the usage pattern shared below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix the ISP violation by creating separate abstractions for LinearRegressionModel, LogisticRegressionModel and ModelEvaluator, based on the usage pattern shared below\n",
    "\n",
    "# Usage:\n",
    "linear_regression_model = LinearRegressionModel(data)\n",
    "linear_regression_model.train()\n",
    "linear_regression_model.make_prediction()\n",
    "\n",
    "logistic_regression_model = LogisticRegressionModel(data)\n",
    "logistic_regression_model.train()\n",
    "logistic_regression_model.make_prediction()\n",
    "\n",
    "model_evaluator = ModelEvaluator(linear_regression_model)\n",
    "model_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this updated design, we've broken down the interface into smaller, more focused interfaces that cater to specific use cases. Each interface has a smaller set of methods that are relevant to that specific use case. This makes it easier for clients to depend on only the interfaces they need, without being forced to depend on a large interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Inversion Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example of a class that violates the Dependency Inversion Principle (DIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DataVisualizer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def visualize(self):\n",
    "        plt.plot(self.data)\n",
    "        plt.xlabel('X Axis')\n",
    "        plt.ylabel('Y Axis')\n",
    "        plt.title('Data Visualization')\n",
    "        plt.show()\n",
    "\n",
    "# Create a sample dataset\n",
    "data = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [2, 4, 6, 8, 10]})\n",
    "\n",
    "# Create an instance of the DataVisualizer class\n",
    "visualizer = DataVisualizer(data['Y'])  # Pass in the 'Y' column of the dataset\n",
    "\n",
    "# Call the visualize method to create the plot\n",
    "visualizer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the DataVisualizer class is tightly coupled to the matplotlib library for visualizing data. This violates the Dependency Inversion Principle, which states that:\n",
    "- High-level modules should not depend on low-level modules. Instead, both should depend on abstractions.\n",
    "- Abstractions should not depend on details. Details should depend on abstractions.\n",
    "\n",
    "In this case, the DataVisualizer class is a high-level module that depends on a low-level module (matplotlib) for a specific implementation of data visualization. This makes it difficult to change or replace the visualization library without affecting the DataVisualizer class.\n",
    "\n",
    "A better design would be to invert the dependencies by introducing an abstraction for data visualization. To fix the DIP violation, implement the MatplotlibVisualizer and DataAnalyzer classes below based on the usage pattern below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DataVisualizer(ABC):\n",
    "    @abstractmethod\n",
    "    def visualize(self, data):\n",
    "        pass\n",
    "\n",
    "class MatplotlibVisualizer(DataVisualizer):\n",
    "    # TODO 1: Implement the visualize method using Matplotlib to create a plot\n",
    "    pass\n",
    "\n",
    "class DataAnalyzer:\n",
    "    # TODO 2: Add a visualizer attribute to the DataAnalyzer class\n",
    "    pass\n",
    "\n",
    "# Create an instance of MatplotlibVisualizer\n",
    "visualizer = MatplotlibVisualizer()\n",
    "\n",
    "# Create an instance of DataAnalyzer, passing in the visualizer\n",
    "analyzer = DataAnalyzer(visualizer)\n",
    "\n",
    "# Generate some sample data\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Call the analyze method on the analyzer, passing in the data\n",
    "analyzer.analyze(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this revised design, we've introduced an abstraction for data visualization (DataVisualizer) and a concrete implementation for it (MatplotlibVisualizer). The DataAnalyzer class depends on the abstraction, rather than the specific implementation. This makes it easier to change or replace the visualization library without affecting the DataAnalyzer class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SD4DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
